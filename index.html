<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Document</title>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/toxicity"></script>
    
</head>
<body>

    <!-- <form action="get" > -->
        <input id="textinput" type="text" placeholder="key in what you want to say"></input>
        <!-- <input type="submit" value="submit"> -->
    <!-- </form> -->
    <div>
        <ul id="responseList">
        </ul>
    </div>


    
    <script>
        var list = document.getElementById('responseList');
        document.getElementById("textinput").addEventListener("change", createListItem)
        
        function createListItem(){
            
            const sentences = document.getElementById("textinput").innerText;
            
            const threshold = 0.8;
    
            // Load the model. Users optionally pass in a threshold and an array of
            // labels to include.
            toxicity.load(threshold).then(model => {
    
                model.classify(sentences).then(predictions => {
                    // `predictions` is an array of objects, one for each prediction head,
                    // that contains the raw probabilities for each input along with the
                    // final prediction in `match` (either `false` or `true`).
                    // If neither prediction exceeds the threshold, `match` is `null`.
                    // list.innerHTML =''
        
                    console.log(predictions);

                    predictions.forEach((elem) => {
                        var entry = document.createElement('li');
                        entry.appendChild(document.createTextNode(`${elem.label}: ${elem.results[0].match}`));
                        list.appendChild(entry);
                    });
                    /*
                    prints:
                    {
                    "label": "identity_attack",
                    "results": [{
                        "probabilities": [0.9659664034843445, 0.03403361141681671],
                        "match": false
                    }]
                    },
                    {
                    "label": "insult",
                    "results": [{
                        "probabilities": [0.08124706149101257, 0.9187529683113098],
                        "match": true
                    }]
                    },
                    ...
                    */
                });
            });
        }



    </script>
</body>
</html>